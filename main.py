import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
import pandas as pd
import os
from typing import List, Dict


class RuBERTSentimentAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä–∞—Å–∞ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ RuBERT –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, 
                 model_name: str = "cointegrated/rubert-tiny-sentiment-balanced",
                 max_length: int = 512,
                 batch_size: int = 32,
                 temperature: float = 1.0,
                 confidence_threshold: float = 0.5,
                 device: str = "auto"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å–æ—Ñ—Ç–º–∞–∫—Å–∞ (–≤–ª–∏—è–µ—Ç –Ω–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
            confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ('cpu', 'cuda', 'auto')
        """
        self.model_name = model_name
        self.max_length = max_length
        self.batch_size = batch_size
        self.temperature = temperature
        self.confidence_threshold = confidence_threshold
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        print(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å {model_name}...")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        
        # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.model = self.model.to(self.device)
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        self.model.eval()
        
        # –ú–∞–ø–ø–∏–Ω–≥ –ª–µ–π–±–ª–æ–≤
        self.label_mapping = {
            0: "NEGATIVE",  # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π
            1: "NEUTRAL",   # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π  
            2: "POSITIVE"   # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π
        }
        
        print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: max_length={max_length}, temperature={temperature}, threshold={confidence_threshold}")
    
    def update_parameters(self, **kwargs):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
        
        –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - temperature: float
        - confidence_threshold: float
        - max_length: int
        - batch_size: int
        """
        for param, value in kwargs.items():
            if hasattr(self, param):
                setattr(self, param, value)
                print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä {param} –æ–±–Ω–æ–≤–ª–µ–Ω –¥–æ {value}")
            else:
                print(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä: {param}")
                
    def get_parameters(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏"""
        return {
            "model_name": self.model_name,
            "max_length": self.max_length,
            "batch_size": self.batch_size,
            "temperature": self.temperature,
            "confidence_threshold": self.confidence_threshold,
            "device": self.device
        }
        
    def chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–∫–µ–Ω–æ–≤
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            chunk_size: –†–∞–∑–º–µ—Ä —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö (–æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –æ—Ç 512)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        """
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –æ–¥–∏–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å
        if len(tokens) <= self.max_length - 2:  # -2 –¥–ª—è [CLS] –∏ [SEP]
            return [text]
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        chunks = []
        for i in range(0, len(tokens), chunk_size):
            chunk_tokens = tokens[i:i + chunk_size]
            chunk_text = self.tokenizer.decode(chunk_tokens, skip_special_tokens=True)
            chunks.append(chunk_text)
        
        return chunks
    
    def predict_sentiment(self, text: str) -> Dict[str, any]:
        """
        –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä–∞—Å–∞ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            padding=True, 
            max_length=self.max_length
        )
        
        # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            outputs = self.model(**inputs)
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            scaled_logits = outputs.logits / self.temperature
            predictions = torch.nn.functional.softmax(scaled_logits, dim=-1)
            
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        probs = predictions.cpu().numpy()[0]
        predicted_class = np.argmax(probs)
        confidence = float(probs[predicted_class])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if confidence < self.confidence_threshold:
            predicted_label = "UNCERTAIN"
        else:
            predicted_label = self.label_mapping[predicted_class]
        
        return {
            "text": text,
            "predicted_label": predicted_label,
            "confidence": confidence,
            "probabilities": {
                "NEGATIVE": float(probs[0]),
                "NEUTRAL": float(probs[1]), 
                "POSITIVE": float(probs[2])
            },
            "is_uncertain": confidence < self.confidence_threshold
        }
    
    def predict_sentiment_with_chunking(self, text: str) -> Dict[str, any]:
        """
        –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä–∞—Å–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        chunks = self.chunk_text(text)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö
        total_tokens = len(self.tokenizer.encode(text, add_special_tokens=False))
        if len(chunks) > 1:
            print(f"üìè –¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç {total_tokens} —Ç–æ–∫–µ–Ω–æ–≤, —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        
        # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥
        if len(chunks) == 1:
            result = self.predict_sentiment(text)
            result["chunks_used"] = 1
            result["chunking_details"] = None
            return result
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
        chunk_results = []
        for i, chunk in enumerate(chunks):
            chunk_result = self.predict_sentiment(chunk)
            chunk_results.append(chunk_result)
            print(f"  –§—Ä–∞–≥–º–µ–Ω—Ç {i+1}/{len(chunks)}: {chunk_result['predicted_label']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {chunk_result['confidence']:.3f})")
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        final_result = self.aggregate_chunk_results(chunk_results, text)
        final_result["chunks_used"] = len(chunks)
        final_result["chunking_details"] = {
            "chunk_results": [
                {
                    "chunk_index": i,
                    "label": r["predicted_label"], 
                    "confidence": r["confidence"]
                } 
                for i, r in enumerate(chunk_results)
            ]
        }
        
        return final_result
    
    def aggregate_chunk_results(self, chunk_results: List[Dict], original_text: str) -> Dict[str, any]:
        """
        –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        
        Args:
            chunk_results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            original_text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        """
        from collections import Counter
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–∏—Å–∫–ª—é—á–∞—è UNCERTAIN)
        valid_predictions = []
        valid_confidences = []
        
        for result in chunk_results:
            if result["predicted_label"] != "UNCERTAIN":
                valid_predictions.append(result["predicted_label"])
                valid_confidences.append(result["confidence"])
        
        # –ï—Å–ª–∏ –≤—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ
        if not valid_predictions:
            avg_confidence = np.mean([r["confidence"] for r in chunk_results])
            return {
                "text": original_text,
                "predicted_label": "UNCERTAIN",
                "confidence": float(avg_confidence),
                "probabilities": {
                    "NEGATIVE": 0.33,
                    "NEUTRAL": 0.34, 
                    "POSITIVE": 0.33
                },
                "is_uncertain": True
            }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≥–æ–ª–æ—Å–∞
        vote_counts = Counter(valid_predictions)
        most_common_label = vote_counts.most_common(1)[0][0]
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–∞–º–æ–≥–æ —á–∞—Å—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞
        same_label_confidences = [
            conf for pred, conf in zip(valid_predictions, valid_confidences)
            if pred == most_common_label
        ]
        avg_confidence = float(np.mean(same_label_confidences))
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –≤–∞–ª–∏–¥–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        agg_probs = {"NEGATIVE": 0.0, "NEUTRAL": 0.0, "POSITIVE": 0.0}
        for result in chunk_results:
            if result["predicted_label"] != "UNCERTAIN":
                for label in agg_probs:
                    agg_probs[label] += result["probabilities"][label]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        total_valid_chunks = len(valid_predictions)
        for label in agg_probs:
            agg_probs[label] /= total_valid_chunks
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if avg_confidence < self.confidence_threshold:
            final_label = "UNCERTAIN"
        else:
            final_label = most_common_label
        
        return {
            "text": original_text,
            "predicted_label": final_label,
            "confidence": avg_confidence,
            "probabilities": agg_probs,
            "is_uncertain": avg_confidence < self.confidence_threshold,
            "voting_details": {
                "votes": dict(vote_counts),
                "total_chunks": len(chunk_results),
                "valid_chunks": total_valid_chunks,
                "winner": most_common_label,
                "winner_votes": vote_counts[most_common_label]
            }
        }
    
    def validate_on_test_data(self, test_csv_path: str = "/home/deck/Desktop/models2/test_simple.csv"):
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            test_csv_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        try:
            print(f"üìã –ó–∞–≥—Ä—É–∂–∞—é —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {test_csv_path}...")
            df = pd.read_csv(test_csv_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            if '–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏' not in df.columns or '–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–∫—Ä–∞—Å' not in df.columns:
                raise ValueError("–í CSV —Ñ–∞–π–ª–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–ª–æ–Ω–∫–∏ '–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏' –∏ '–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–∫—Ä–∞—Å'")
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å
            predictions = []
            true_labels = []
            detailed_results = []
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º enumerate –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞
            for i, (idx, row) in enumerate(df.iterrows()):
                text = row['–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏']
                true_label = row['–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–∫—Ä–∞—Å']
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ —Å—Ç—Ä–æ–∫—É –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—É—Å—Ç–æ—Ç—É
                text = str(text).strip()
                true_label = str(true_label).strip()
                
                if not text or text == 'nan':
                    print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é –Ω–æ–≤–æ—Å—Ç—å {i+1}/{len(df)} - –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
                    continue
                
                print(f"–í–∞–ª–∏–¥–∏—Ä—É—é –Ω–æ–≤–æ—Å—Ç—å {i+1}/{len(df)}...")
                
                result = self.predict_sentiment_with_chunking(text)
                predicted_label = result['predicted_label']
                confidence = result['confidence']
                
                predictions.append(predicted_label)
                true_labels.append(true_label)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                is_correct = predicted_label == true_label
                detailed_results.append({
                    'text_preview': text[:100] + "..." if len(text) > 100 else text,
                    'true_label': true_label,
                    'predicted_label': predicted_label,
                    'confidence': confidence,
                    'is_correct': is_correct
                })
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                status = "‚úÖ" if is_correct else "‚ùå"
                print(f"  {status} –û–∂–∏–¥–∞–ª–æ—Å—å: {true_label}, –ü–æ–ª—É—á–µ–Ω–æ: {predicted_label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f})")
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            from collections import Counter
            
            correct_predictions = sum(1 for i in range(len(predictions)) if predictions[i] == true_labels[i])
            total_predictions = len(predictions)
            accuracy = correct_predictions / total_predictions
            
            # –ü–æ–¥—Å—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º
            true_counter = Counter(true_labels)
            pred_counter = Counter(predictions)
            
            # Confusion matrix (–ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è)
            confusion_data = {}
            for true_label in set(true_labels):
                confusion_data[true_label] = {}
                for pred_label in set(predictions):
                    confusion_data[true_label][pred_label] = sum(
                        1 for i in range(len(predictions)) 
                        if true_labels[i] == true_label and predictions[i] == pred_label
                    )
            
            validation_results = {
                'accuracy': accuracy,
                'correct_predictions': correct_predictions,
                'total_predictions': total_predictions,
                'true_distribution': dict(true_counter),
                'predicted_distribution': dict(pred_counter),
                'confusion_matrix': confusion_data,
                'detailed_results': detailed_results
            }
            
            self.print_validation_results(validation_results)
            return validation_results
            
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª {test_csv_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            return None
    
    def print_validation_results(self, results: Dict):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        print("\n" + "="*80)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò –ú–û–î–ï–õ–ò")
        print("="*80)
        
        accuracy = results['accuracy']
        correct = results['correct_predictions']
        total = results['total_predictions']
        
        print(f"\nüéØ –û–ë–©–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {accuracy:.3f} ({correct}/{total})")
        print(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {accuracy*100:.1f}%")
        
        print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ò–°–¢–ò–ù–ù–´–• –ú–ï–¢–û–ö:")
        for label, count in results['true_distribution'].items():
            percentage = (count / total) * 100
            print(f"  {label}: {count} ({percentage:.1f}%)")
        
        print(f"\nüîÆ –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ù–´–• –ú–ï–¢–û–ö:")
        for label, count in results['predicted_distribution'].items():
            percentage = (count / total) * 100
            print(f"  {label}: {count} ({percentage:.1f}%)")
        
        print(f"\nüìã –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        header = "–ò—Å—Ç–∏–Ω–Ω–æ–µ\\–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ"
        print(f"{header:<25}", end="")
        all_labels = sorted(set(list(results['true_distribution'].keys()) + list(results['predicted_distribution'].keys())))
        for label in all_labels:
            print(f"{label:>10}", end="")
        print()
        
        for true_label in all_labels:
            print(f"{true_label:<25}", end="")
            for pred_label in all_labels:
                count = results['confusion_matrix'].get(true_label, {}).get(pred_label, 0)
                print(f"{count:>10}", end="")
            print()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏
        errors = [r for r in results['detailed_results'] if not r['is_correct']]
        if errors:
            print(f"\n‚ùå –û–®–ò–ë–ö–ò –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò ({len(errors)} –∏–∑ {total}):")
            for i, error in enumerate(errors[:5], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
                print(f"\n{i}. {error['text_preview']}")
                print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: {error['true_label']}")
                print(f"   –ü–æ–ª—É—á–µ–Ω–æ: {error['predicted_label']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {error['confidence']:.3f})")
            
            if len(errors) > 5:
                print(f"\n... –∏ –µ—â–µ {len(errors) - 5} –æ—à–∏–±–æ–∫")
        else:
            print(f"\nüéâ –í–°–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –ü–†–ê–í–ò–õ–¨–ù–´–ï!")

    def analyze_news_batch(self, news_texts: List[str]) -> List[Dict[str, any]]:
        """
        –ê–Ω–∞–ª–∏–∑ —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
        
        Args:
            news_texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        """
        results = []
        for i, text in enumerate(news_texts):
            print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –Ω–æ–≤–æ—Å—Ç—å {i+1}/{len(news_texts)}...")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤
            result = self.predict_sentiment_with_chunking(text)
            results.append(result)
        
        return results
    
    def print_results(self, results: List[Dict[str, any]]):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        print("\n" + "="*80)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–û–ì–û –û–ö–†–ê–°–ê –ù–û–í–û–°–¢–ï–ô")
        print("="*80)
        
        for i, result in enumerate(results, 1):
            print(f"\nüì∞ –ù–û–í–û–°–¢–¨ {i}:")
            print(f"–¢–µ–∫—Å—Ç: {result['text'][:250]}{'...' if len(result['text']) > 250 else ''}")
            print(f"üéØ –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–∫—Ä–∞—Å: {result['predicted_label']}")
            print(f"üîÆ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.3f}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–∏
            if result.get('chunks_used', 1) > 1:
                print(f"üìè –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {result['chunks_used']}")
                if 'voting_details' in result:
                    votes = result['voting_details']['votes']
                    winner = result['voting_details']['winner']
                    print(f"üìä –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –ø–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º: {votes}")
                    print(f"üèÜ –ü–æ–±–µ–¥–∏–≤—à–∏–π –∫–ª–∞—Å—Å: {winner} ({result['voting_details']['winner_votes']} –≥–æ–ª–æ—Å–æ–≤)")
            
            print(f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:")
            for label, prob in result['probabilities'].items():
                print(f"   {label}: {prob:.3f}")
            
            if result.get('is_uncertain', False):
                print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏!")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—é
        chunked_count = sum(1 for r in results if r.get('chunks_used', 1) > 1)
        if chunked_count > 0:
            total_chunks = sum(r.get('chunks_used', 1) for r in results)
            avg_chunks = total_chunks / len(results)
            print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ß–ê–ù–ö–û–í–ê–ù–ò–Ø:")
            print(f"–¢–µ–∫—Å—Ç–æ–≤ —Å —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ–º: {chunked_count}/{len(results)}")
            print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Ç–µ–∫—Å—Ç: {avg_chunks:.1f}")


def load_news_from_csv(file_path: str, sample_size: int = 5) -> List[str]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ CSV —Ñ–∞–π–ª–∞
    
    Args:
        file_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
        sample_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –≤—ã–±–æ—Ä–∫–∏
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    try:
        print(f"üìÅ –ó–∞–≥—Ä—É–∂–∞—é –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ {file_path}...")
        df = pd.read_csv(file_path)
        
        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º (–≤–æ–∑–º–æ–∂–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è)
        text_columns = ['text', 'title', 'content', 'news', 'article', 'body']
        text_column = None
        
        for col in text_columns:
            if col in df.columns:
                text_column = col
                break
        
        if text_column is None:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É
            for col in df.columns:
                if df[col].dtype == 'object':
                    text_column = col
                    break
        
        if text_column is None:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–æ–ª–æ–Ω–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º –≤ CSV —Ñ–∞–π–ª–µ")
        
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º: '{text_column}'")
        print(f"üìä –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ñ–∞–π–ª–µ: {len(df)}")
        
        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –±–µ—Ä–µ–º –≤—ã–±–æ—Ä–∫—É
        texts = df[text_column].dropna().astype(str).tolist()
        
        if len(texts) < sample_size:
            sample_size = len(texts)
            print(f"‚ö†Ô∏è –í —Ñ–∞–π–ª–µ –º–µ–Ω—å—à–µ —Ç–µ–∫—Å—Ç–æ–≤ —á–µ–º –∑–∞–ø—Ä–æ—à–µ–Ω–æ, –±–µ—Ä–µ–º {sample_size}")
        
        # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É
        import random
        sample_texts = random.sample(texts, sample_size)
        
        print(f"‚úÖ –í—ã–±—Ä–∞–Ω–æ {len(sample_texts)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return sample_texts
        
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return []
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return []


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä–∞—Å–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    print("=" * 50)
    
    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
    print("\nü§ñ –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:")
    print("  1. –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (cointegrated/rubert-tiny-sentiment-balanced)")
    print("  2. –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (./rubert_finetuned)")
    
    model_choice = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –º–æ–¥–µ–ª–∏ (1/2) [–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1]: ").strip()
    
    if model_choice == '2':
        if os.path.exists('./rubert_finetuned'):
            model_name = './rubert_finetuned'
            print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
        else:
            print("‚ö†Ô∏è  –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è")
            model_name = 'cointegrated/rubert-tiny-sentiment-balanced'
    else:
        model_name = 'cointegrated/rubert-tiny-sentiment-balanced'
        print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
    print(f"\n‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    analyzer = RuBERTSentimentAnalyzer(
        model_name=model_name,
        temperature=1.0,
        confidence_threshold=0.5,
        max_length=512
    )
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    print(f"\nüí¨ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º:")
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:")
    print("  - –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    print("  - 'params' - –ø–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    print("  - 'model' - –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å")
    print("  - 'set <–ø–∞—Ä–∞–º–µ—Ç—Ä> <–∑–Ω–∞—á–µ–Ω–∏–µ>' - –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä")
    print("  - 'validate' - –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ test_simple.csv")
    print("  - 'exit' - –≤—ã—Ö–æ–¥")
    print("\n–ü—Ä–∏–º–µ—Ä—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
    print("  set temperature 0.8")
    print("  set confidence_threshold 0.7")
    print("  set max_length 128")
    
    while True:
        user_input = input("\n>>> ").strip()
        
        if user_input.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
            break
        elif user_input.lower() == 'params':
            params = analyzer.get_parameters()
            print("–¢–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
            for key, value in params.items():
                print(f"  {key}: {value}")
            print(f"  model: {analyzer.model_name}")
        elif user_input.lower() == 'model':
            print("\nü§ñ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:")
            print("  1. –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (cointegrated/rubert-tiny-sentiment-balanced)")
            print("  2. –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (./rubert_finetuned)")
            
            new_model_choice = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –º–æ–¥–µ–ª–∏ (1/2): ").strip()
            
            if new_model_choice == '2':
                if os.path.exists('./rubert_finetuned'):
                    new_model_name = './rubert_finetuned'
                    print("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
                else:
                    print("‚ùå –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
                    continue
            elif new_model_choice == '1':
                new_model_name = 'cointegrated/rubert-tiny-sentiment-balanced'
                print("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
                continue
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
            print("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
            old_params = analyzer.get_parameters()
            analyzer = RuBERTSentimentAnalyzer(
                model_name=new_model_name,
                temperature=old_params['temperature'],
                confidence_threshold=old_params['confidence_threshold'],
                max_length=old_params['max_length']
            )
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {new_model_name}")
        elif user_input.lower() == 'validate':
            print("\nüîç –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            validation_results = analyzer.validate_on_test_data()
            if validation_results:
                print(f"\nüìä –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –¢–æ—á–Ω–æ—Å—Ç—å: {validation_results['accuracy']*100:.1f}%")
        elif user_input.lower().startswith('set '):
            parts = user_input.split()
            if len(parts) == 3:
                param_name = parts[1]
                try:
                    param_value = float(parts[2]) if '.' in parts[2] else int(parts[2])
                    analyzer.update_parameters(**{param_name: param_value})
                except ValueError:
                    print("–û—à–∏–±–∫–∞: –∑–Ω–∞—á–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —á–∏—Å–ª–æ–º")
            else:
                print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: set <–ø–∞—Ä–∞–º–µ—Ç—Ä> <–∑–Ω–∞—á–µ–Ω–∏–µ>")
        elif user_input:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤–≤–æ–¥ –ø—É—Ç—ë–º –∫ —Ñ–∞–π–ª—É
            if user_input.endswith('.txt') and os.path.exists(user_input):
                print(f"\nüìÑ –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞: {user_input}")
                with open(user_input, 'r', encoding='utf-8') as f:
                    text = f.read()
                print(f"üìè –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                text = user_input
            
            result = analyzer.predict_sentiment_with_chunking(text)
            print(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['predicted_label']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.3f})")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–∏
            if result.get('chunks_used', 1) > 1:
                print(f"üìè –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {result['chunks_used']}")
                if 'voting_details' in result:
                    print(f"üìä –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ: {result['voting_details']['votes']}")
            
            if result['is_uncertain']:
                print("‚ö†Ô∏è  –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ UNCERTAIN")
    
    print("\nüëã –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == "__main__":
    main()
