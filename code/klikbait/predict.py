"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∏–∫–±–µ–π—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
"""
import argparse
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch


class ClickbaitDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –∫–ª–∏–∫–±–µ–π—Ç–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
    
    def __init__(self, model_path="my_awesome_model"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        
        Args:
            model_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        import os
        
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å checkpoint'–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π
        if os.path.isdir(model_path) and not os.path.exists(os.path.join(model_path, "config.json")):
            checkpoints = [d for d in os.listdir(model_path) if d.startswith("checkpoint-")]
            if checkpoints:
                # –ë–µ—Ä–µ–º checkpoint —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –Ω–æ–º–µ—Ä–æ–º
                latest_checkpoint = max(checkpoints, key=lambda x: int(x.split("-")[1]))
                model_path = os.path.join(model_path, latest_checkpoint)
                print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è checkpoint: {latest_checkpoint}")
        
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.classifier = pipeline(
            "text-classification",
            model=self.model,
            tokenizer=self.tokenizer,
            device=-1  # CPU
        )
        print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    
    def predict(self, text):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        
        Args:
            text: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏
            
        Returns:
            dict: {"label": "–∫–ª–∏–∫–±–µ–π—Ç"/"–Ω–µ –∫–ª–∏–∫–±–µ–π—Ç", "score": 0.95}
        """
        result = self.classifier(text)[0]
        return result
    
    def predict_batch(self, texts):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            
        Returns:
            list: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        """
        return self.classifier(texts)
    
    def is_clickbait(self, text, threshold=0.5):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–ª–∏–∫–±–µ–π—Ç–æ–º
        
        Args:
            text: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏
            threshold: –ø–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            
        Returns:
            bool: True –µ—Å–ª–∏ –∫–ª–∏–∫–±–µ–π—Ç
        """
        result = self.predict(text)
        return result['label'] == '–∫–ª–∏–∫–±–µ–π—Ç' and result['score'] >= threshold


def main():
    parser = argparse.ArgumentParser(description="–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∏–∫–±–µ–π—Ç–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")
    parser.add_argument("text", nargs="*", help="–¢–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    parser.add_argument("-f", "--file", help="–§–∞–π–ª —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)")
    parser.add_argument("-m", "--model", default="my_awesome_model", help="–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏")
    parser.add_argument("-t", "--threshold", type=float, default=0.5, help="–ü–æ—Ä–æ–≥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–µ–∫—Ç–æ—Ä
    detector = ClickbaitDetector(args.model)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤
    texts = []
    if args.file:
        # –ò–∑ —Ñ–∞–π–ª–∞
        with open(args.file, 'r', encoding='utf-8') as f:
            texts = [line.strip() for line in f if line.strip()]
    elif args.text:
        # –ò–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        texts = [' '.join(args.text)]
    else:
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
        print("\n=== –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º ===")
        print("–í–≤–µ–¥–∏—Ç–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞):\n")
        while True:
            try:
                text = input("> ").strip()
                if text.lower() in ['exit', 'quit', 'q']:
                    break
                if not text:
                    continue
                    
                result = detector.predict(text)
                is_clickbait = result['label'] == '–∫–ª–∏–∫–±–µ–π—Ç'
                confidence = result['score'] * 100
                
                print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['label'].upper()}")
                print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1f}%")
                
                if is_clickbait:
                    print("   üö® –≠—Ç–æ –∫–ª–∏–∫–±–µ–π—Ç!\n")
                else:
                    print("   ‚úÖ –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫\n")
                    
            except KeyboardInterrupt:
                print("\n\n–í—ã—Ö–æ–¥...")
                break
        return
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(texts)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤...\n")
    results = detector.predict_batch(texts)
    
    for text, result in zip(texts, results):
        is_clickbait = result['label'] == '–∫–ª–∏–∫–±–µ–π—Ç'
        confidence = result['score'] * 100
        
        icon = "üö®" if is_clickbait else "‚úÖ"
        print(f"{icon} [{result['label'].upper():12}] ({confidence:5.1f}%) {text}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    clickbait_count = sum(1 for r in results if r['label'] == '–∫–ª–∏–∫–±–µ–π—Ç')
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {clickbait_count}/{len(texts)} –∫–ª–∏–∫–±–µ–π—Ç–æ–≤ ({clickbait_count/len(texts)*100:.1f}%)")


if __name__ == "__main__":
    main()
